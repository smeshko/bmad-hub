---
name: 'step-02-execute'
description: 'Execute all configured test commands and collect results'

# Path Definitions
workflow_path: '{project-root}/_bmad/bmm/workflows/4-implementation/test'

# File References
thisStepFile: '{workflow_path}/steps/step-02-execute.md'
fixStepFile: '{workflow_path}/steps/step-03-fix.md'
reportStepFile: '{workflow_path}/steps/step-04-report.md'
workflowFile: '{workflow_path}/workflow.md'
---

# Step 2: Execute Test Commands

## STEP GOAL:

To execute all configured test commands, collect results, and route to fix step if failures occur.

## MANDATORY EXECUTION RULES (READ FIRST):

### Universal Rules:

- ğŸ“– CRITICAL: Read the complete step file before taking any action
- ğŸ¯ This is an action workflow - execute efficiently
- ğŸ”„ Run ALL commands - do not fail fast

### Role Reinforcement:

- âœ… You are a test runner
- âœ… Execute every enabled command
- âœ… Collect all output for analysis

### Step-Specific Rules:

- ğŸ¯ Execute ALL enabled commands regardless of failures
- ğŸš« FORBIDDEN to stop on first failure
- âœ… Capture stdout and stderr for each command
- âœ… Track pass/fail status for each command

## EXECUTION PROTOCOLS:

- ğŸ¯ Execute ALL enabled commands regardless of individual failures
- ğŸ’¾ Track pass/fail/timing for each command
- ğŸ“– Capture stdout and stderr for failed commands
- ğŸš« FORBIDDEN to stop on first failure (run all)

## CONTEXT BOUNDARIES:

- Available context: Enabled commands and config from step 1
- Focus: Command execution and result collection
- Limits: Execute only, no fixing in this step
- Dependencies: Valid test configuration from step 1

## EXECUTION SEQUENCE:

### 1. Initialize Results Tracking

Create results object:
```
results = {
  passed: [],
  failed: [],
  skipped: [],
  totalTime: 0
}
```

### 2. Execute Commands in Order

Run commands in this order (if enabled):

#### A. Pre-Commands
For each command in `pre_commands`:
- Execute command
- Record result (pass/fail)
- Capture output
- Continue regardless of result

#### B. Unit Tests
If `unit.enabled`:
- Execute `unit.command`
- Record result
- Capture output

#### C. E2E Tests
If `e2e.enabled`:
- Execute `e2e.command`
- Record result
- Capture output

#### D. Lint
If `lint.enabled`:
- Execute `lint.command`
- Record result
- Capture output

#### E. Type Check
If `typecheck.enabled`:
- Execute `typecheck.command`
- Record result
- Capture output

#### F. Custom Commands
For each command in `custom`:
- Execute command
- Record result
- Capture output

### 3. Display Execution Progress

For each command, display:
```
Running: [command name]...
[command output - abbreviated if long]
Result: âœ… PASS / âŒ FAIL
```

### 4. Compile Results Summary

After all commands complete:

"**Execution Complete**

| Command | Status | Duration |
|---------|--------|----------|
| [name] | âœ…/âŒ | [time] |
| ... | ... | ... |

**Summary:** [X] passed, [Y] failed, [Z] skipped
**Total Time:** [duration]"

### 5. Route Based on Results

#### IF all passed:
Display: "All tests passed!"
Load, read entire file, then execute `{reportStepFile}`

#### IF any failed:
Display: "Some tests failed. Attempting automatic fixes..."
Store failed commands and their error output in context
Load, read entire file, then execute `{fixStepFile}`

---

## ğŸš¨ SYSTEM SUCCESS/FAILURE METRICS

### âœ… SUCCESS:

- All enabled commands executed
- Results properly tracked
- Output captured for failed commands
- Correct routing to next step

### âŒ SYSTEM FAILURE:

- Stopping on first failure
- Not capturing error output
- Not tracking all results
- Wrong routing based on results

**Master Rule:** Run ALL commands and collect ALL results before proceeding.
