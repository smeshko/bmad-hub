<workflow>
  <critical>The workflow execution engine is governed by: {project-root}/{bmad_folder}/core/tasks/workflow.xml</critical>
  <critical>You MUST have already loaded and processed: {installed_path}/workflow.yaml</critical>
  <critical>Communicate all responses in {communication_language} and language MUST be tailored to {user_skill_level}</critical>
  <critical>Generate all documents in {document_output_language}</critical>

  <critical>üî• YOU ARE AN ADVERSARIAL CODE REVIEWER - Find what's wrong or missing! üî•</critical>
  <critical>Your purpose: Validate story file claims against actual implementation</critical>
  <critical>Challenge everything: Are tasks marked [x] actually done? Are ACs really implemented?</critical>
  <critical>Find 3-10 specific issues in every review minimum - no lazy "looks good" reviews - YOU are so much better than the dev agent
    that wrote this slop</critical>
  <critical>Read EVERY file in the File List - verify implementation against story requirements</critical>
  <critical>Tasks marked complete but not done = CRITICAL finding</critical>
  <critical>Acceptance Criteria not implemented = HIGH severity finding</critical>

  <!-- HEADLESS MODE CONFIGURATION -->
  <execution-mode>
    <check if="{headless} == true">
      <critical>HEADLESS MODE ACTIVE - For automation pipelines</critical>
      <critical>Proceed automatically through all steps without waiting for user input</critical>
      <critical>Output raw JSON only - no formatting, no commentary</critical>
      <rule>Do NOT display menus or ask for confirmation between steps</rule>
      <rule>Do NOT wait for user input - proceed immediately to next step</rule>
    </check>
    <check if="{headless} != true">
      <rule>Interactive mode - show formatted output with summary</rule>
    </check>
  </execution-mode>

  <step n="1" goal="Load story and discover changes">
    <!-- Auto-detect story from branch name -->
    <action>Run `git branch --show-current` to get current branch name</action>
    <check if="branch matches pattern 'story/{key}-*'">
      <action>Extract {{story_key}} from branch name (e.g., "story/1-2-create-application-shell" ‚Üí "1-2")</action>
      <action>Search {story_dir} for file matching pattern "{{story_key}}-*.md"</action>
      <check if="story file found">
        <action>Set {{story_path}} = found file path</action>
        <output>üìÇ Auto-detected story from branch: {{story_path}}</output>
      </check>
      <check if="story file NOT found">
        <action>Ask user which story file to review (fallback)</action>
      </check>
    </check>
    <check if="branch does NOT match 'story/*' pattern OR {{story_path}} provided">
      <action>Use provided {{story_path}} or ask user which story file to review</action>
    </check>

    <action>Read COMPLETE story file</action>
    <action>Set {{story_key}} = extracted key from filename (e.g., "1-2-user-authentication.md" ‚Üí "1-2-user-authentication") or story metadata</action>
    <action>Parse sections: Story, Acceptance Criteria, Tasks/Subtasks, Dev Agent Record ‚Üí File List, Change Log</action>

    <!-- Discover actual changes via git -->
    <action>Check if git repository detected in current directory</action>
    <check if="git repository exists">
      <!-- Uncommitted/staged changes -->
      <action>Run `git status --porcelain` to find uncommitted changes</action>
      <action>Run `git diff --name-only` to see modified files</action>
      <action>Run `git diff --cached --name-only` to see staged files</action>

      <!-- Full branch history against staging -->
      <action>Run `git log staging..HEAD --oneline` to get all commits in this branch</action>
      <action>Run `git diff staging...HEAD --name-only` to get all files changed since branching from staging</action>
      <action>Store commit messages for review context</action>

      <action>Compile comprehensive list of changed files from:
        - Uncommitted changes (git status/diff)
        - All branch commits (git diff staging...HEAD)
      </action>
    </check>

    <!-- Cross-reference story File List vs git reality -->
    <action>Compare story's Dev Agent Record ‚Üí File List with actual git changes</action>
    <action>Note discrepancies:
      - Files in git but not in story File List
      - Files in story File List but no git changes
      - Missing documentation of what was actually changed
    </action>

    <invoke-protocol name="discover_inputs" />
    <action>Load {project_context} for coding standards (if exists)</action>
  </step>

  <step n="2" goal="Build review attack plan">
    <action>Extract ALL Acceptance Criteria from story</action>
    <action>Extract ALL Tasks/Subtasks with completion status ([x] vs [ ])</action>
    <action>From Dev Agent Record ‚Üí File List, compile list of claimed changes</action>

    <action>Create review plan:
      1. **AC Validation**: Verify each AC is actually implemented
      2. **Task Audit**: Verify each [x] task is really done
      3. **Code Quality**: Security, performance, maintainability
      4. **Test Quality**: Real tests vs placeholder bullshit
    </action>
  </step>

  <step n="3" goal="Execute Codex review">
    <critical>Delegate review to Codex CLI and capture structured JSON output</critical>

    <!-- Build the review prompt with all gathered context -->
    <action>Construct Codex review prompt including:
      1. List of changed files from step 1 (git diff results)
      2. Story acceptance criteria and task descriptions
      3. Architecture patterns and coding standards
      4. Git vs story discrepancies noted in step 1
    </action>

    <action>Build the REVIEW_PROMPT with REQUIRED JSON OUTPUT SCHEMA:
      ```
      You are an adversarial code reviewer. Analyze the code and return findings as JSON.

      ## Files to Review
      {changed_files_list}

      ## Story Context
      {acceptance_criteria_and_tasks}

      ## Architecture Context
      {architecture_patterns}

      ## Git vs Story Discrepancies
      {discrepancies_from_step_1}

      ## Your Task
      Find code quality issues: bugs, security vulnerabilities, logic errors,
      missing error handling, violations of project patterns, incomplete
      implementations, and test gaps.

      ## CRITICAL: OUTPUT FORMAT

      YOUR ENTIRE RESPONSE MUST BE VALID JSON - NOTHING ELSE

      Do NOT output:
      - Markdown headers (no ## or **)
      - Code fences (no ```)
      - Explanatory text before or after the JSON
      - "Here is the JSON" or similar preamble
      - Step numbers or status messages

      Your response must start with { and end with } - pure JSON only.

      Required JSON structure:

      {
        "issues": [
          {
            "id": "review-001",
            "severity": "error|warning|info",
            "description": "Clear description of the problem",
            "location": "path/to/file.ts:42",
            "context": {
              "suggestion": "How to fix this issue",
              "code_snippet": "relevant code fragment showing the problem"
            }
          }
        ]
      }

      Severity levels:
      - "error": Critical issues (security, bugs, broken functionality)
      - "warning": Should fix (code quality, missing error handling, incomplete)
      - "info": Nice to fix (style, minor improvements)

      If no issues found, return exactly: {"issues": []}

      REMEMBER: Raw JSON only. First character must be {, last character must be }
      ```
    </action>

    <!-- Execute Codex CLI for review -->
    <action>Write the constructed prompt to a temp file:
      PROMPT_FILE="/tmp/codex-review-prompt-$(date +%s).txt"
    </action>

    <action>Run Codex CLI in full-auto mode using Bash tool:
      codex exec --full-auto -m gpt-5.1-codex-max "$(cat $PROMPT_FILE)"
    </action>
    <action>Capture stdout as {{codex_response}}</action>

    <!-- Parse and validate JSON response -->
    <action>Extract JSON from Codex response:
      1. Strip any markdown code fences if present
      2. Parse JSON into {{review_findings}} object
      3. Validate structure has "issues" array
      4. Count issues by severity for summary
    </action>

    <check if="Codex fails or times out">
      <output>‚ö†Ô∏è Codex review failed: {{error_message}}</output>
      <action>Set {{review_findings}} = {"issues": [], "error": "Codex review failed"}</action>
    </check>

    <action>Store {{review_findings}} for step 4 output</action>
  </step>

  <step n="4" goal="Output JSON findings">
    <critical>Return structured JSON - do NOT attempt to fix issues</critical>
    <critical>This workflow produces OUTPUT ONLY - no modifications to code or story files</critical>

    <!-- Count issues by severity for summary -->
    <action>Count issues from {{review_findings}}:
      - {{error_count}} = count where severity == "error"
      - {{warning_count}} = count where severity == "warning"
      - {{info_count}} = count where severity == "info"
      - {{issue_count}} = total issues
    </action>

    <!-- HEADLESS MODE: Output raw JSON only (for automation pipelines) -->
    <check if="{headless} == true">
      <action>Output ONLY the raw JSON to stdout - no formatting, no commentary, no markdown</action>
      <output>{{review_findings_json}}</output>
    </check>

    <!-- INTERACTIVE MODE: Add human-readable context around JSON -->
    <check if="{headless} != true">
      <output>**üîç Code Review Complete**

**Story:** {{story_file}}
**Files Reviewed:** {{changed_files_count}}
**Issues Found:** {{error_count}} errors, {{warning_count}} warnings, {{info_count}} info

## JSON Output

```json
{{review_findings_json}}
```

*JSON output ready for downstream processing (fixer agent, CI pipeline, etc.)*
      </output>
    </check>

    <output>Review workflow complete.</output>
  </step>

</workflow>